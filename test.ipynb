{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial particles population of size N drawn from the initial sampling distribution\n",
    "x = np.random.uniform(low=-5, high=5, size=N)\n",
    "# eps_t for the e_g scale\n",
    "e = [1000,10,5,2,1,0.5,0.2,0.1,0.05,0.05]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "# Mutation kernel :\n",
    "def M(t,x, tau = 1) :\n",
    "    return np.sum(W*norm.pdf(x[t],loc = x[t-1], scale = tau**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "# initalization : \n",
    "t = 1\n",
    "# initial particles population of size N drawn from the initial sampling distribution\n",
    "x = np.random.uniform(low=-5, high=5, size=N)\n",
    "W = np.random.normal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of the \"observed data\" y :\n",
    "n = 200\n",
    "params = {'alpha': 1.7, 'beta' : 0.9, 'gamma' :10, 'delta' : 10}\n",
    "y = univ_alpha_stable_sampler(params,size=n,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_data(theta):\n",
    "    \"\"\"Function to simulate data.\"\"\"\n",
    "    # Implement your data generation process here\n",
    "    pass\n",
    "\n",
    "def distance_function(data, simulated_data):\n",
    "    \"\"\"Compute the distance between observed and simulated data.\"\"\"\n",
    "    # Implement your distance function here\n",
    "    pass\n",
    "\n",
    "def prior_sampler():\n",
    "    \"\"\"Function to sample from the prior distribution.\"\"\"\n",
    "    # Implement your prior sampler here\n",
    "    alpha = np.random.uniform(low=1.1, high=2, size=N)\n",
    "    beta = np.random.uniform(low=-1, high=1, size=N)\n",
    "    gamma = np.random.uniform(low=0, high=300, size=N)\n",
    "    delta = np.random.uniform(low=-300, high=300, size=N)\n",
    "\n",
    "    #theta = {\"alpha\" : alpha, \"beta\": beta, \"gamma\": gamma, \"delta\": delta}\n",
    "    theta = np.array([alpha, beta, gamma, delta]).T\n",
    "    theta = np.array([{'alpha' : theta[k][0], 'beta' : theta[k][1], 'gamma': theta[k][2], 'delta': theta[k][3]} for k in range(N)])\n",
    "    return theta\n",
    "    \n",
    "def prc_abc_smc(n_particles, n_iterations, epsilon_schedule):\n",
    "    \"\"\"\n",
    "    Population Rejection Control ABC SMC algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        n_particles (int): Number of particles (i.e., samples) in each iteration.\n",
    "        n_iterations (int): Number of SMC iterations.\n",
    "        epsilon_schedule (list): Schedule of tolerance levels.\n",
    "\n",
    "    Returns:\n",
    "        list: List of accepted particles in the final iteration.\n",
    "    \"\"\"\n",
    "    particles = []\n",
    "    weights = np.zeros(n_particles)\n",
    "    accepted_particles = []\n",
    "\n",
    "    for t in range(n_iterations):\n",
    "        epsilon = epsilon_schedule[t]\n",
    "        new_particles = []\n",
    "\n",
    "        for i in range(n_particles):\n",
    "            theta = prior_sampler()  # Sample from the prior distribution\n",
    "            simulated_data = simulate_data(theta)  # Simulate data\n",
    "            distance = distance_function(observed_data, simulated_data)  # Compute distance\n",
    "            if distance <= epsilon:\n",
    "                new_particles.append(theta)\n",
    "                weights[i] = 1\n",
    "\n",
    "        particles = new_particles\n",
    "        if len(particles) == 0:\n",
    "            break\n",
    "\n",
    "        if t < n_iterations - 1:\n",
    "            # Update weights for the next iteration\n",
    "            weights = weights / np.sum(weights)\n",
    "            resampled_indices = np.random.choice(range(len(particles)), size=n_particles, replace=True, p=weights)\n",
    "            particles = [particles[i] for i in resampled_indices]\n",
    "            weights = np.zeros(n_particles)\n",
    "\n",
    "    accepted_particles.extend(particles)\n",
    "\n",
    "    return accepted_particles\n",
    "\n",
    "# Example usage:\n",
    "# Set parameters\n",
    "n_particles = 1000\n",
    "n_iterations = 5\n",
    "epsilon_schedule = [0.1, 0.08, 0.06, 0.04, 0.02]  # Example schedule, adjust as needed\n",
    "\n",
    "# Call the algorithm\n",
    "accepted_samples = prc_abc_smc(n_particles, n_iterations, epsilon_schedule)\n",
    "\n",
    "# Output accepted samples\n",
    "print(\"Accepted samples:\", accepted_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import levy_stable, uniform\n",
    "\n",
    "def prior_sample():\n",
    "    # Sample from prior distribution\n",
    "    # For univariate alpha-stable models, sample each parameter from its respective uniform distribution\n",
    "    alpha = uniform.rvs(1.1, 0.9)  # U[1.1, 2]\n",
    "    beta = uniform.rvs(-1, 2)  # U[-1, 1]\n",
    "    gamma = uniform.rvs(0, 300)  # U[0, 300]\n",
    "    delta = uniform.rvs(-300, 600)  # U[-300, 300]\n",
    "    return alpha, beta, gamma, delta\n",
    "\n",
    "def simulate_data(theta):\n",
    "    # Simulate data from the model given parameters theta\n",
    "    alpha, beta, gamma, delta = theta\n",
    "    return levy_stable.rvs(alpha, beta, loc=delta, scale=gamma, size=200)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.511771835806433, 0.7335787906417044, 281.0439798029078, -133.32014593910085)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = prior_sample()\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.67857040e+02,  1.29677739e+03, -4.74859852e+02, -6.84025433e+02,\n",
       "       -7.03720992e+02, -4.88980862e+02, -7.34831957e+02, -2.25284972e+02,\n",
       "       -3.38071156e+02, -2.20153267e+02,  5.26863012e+02, -6.94474565e+02,\n",
       "       -1.56776142e+02, -7.08795241e+02, -2.18602813e+02, -2.58280726e+02,\n",
       "       -3.90812590e+02, -6.74366129e+02,  3.30187805e+02, -4.37488713e+02,\n",
       "       -8.18119490e+02, -3.01811579e+02, -2.94513911e+02, -9.37511132e+02,\n",
       "        7.24389238e+02, -2.86950808e+02,  2.43945973e+02, -4.11415016e+02,\n",
       "       -4.24865176e+02,  3.55367240e+02,  9.54033495e+01,  3.21843447e+02,\n",
       "        1.62106531e+03, -6.81042967e+02, -6.35534519e+02,  3.91396420e+02,\n",
       "       -5.17078509e+02, -2.25475470e+02, -5.72981456e+02, -5.33919170e+01,\n",
       "        1.84326133e+02, -8.12838510e+02, -7.21044844e+02,  1.62825243e+02,\n",
       "       -7.55778535e+02, -4.11046510e+02, -1.05358719e+02, -1.93424488e+02,\n",
       "       -7.50372959e+02, -5.31027766e+02,  2.92037476e+03,  3.64205963e+01,\n",
       "       -9.18627636e+02, -3.12996483e+02, -7.86590172e+02, -1.37809071e+03,\n",
       "       -2.97144783e+02, -2.65986838e+02, -2.89285231e+02, -3.13815479e+02,\n",
       "       -5.06538128e+02, -1.77143424e+02, -1.03662080e+03, -3.32769126e+02,\n",
       "       -4.17347465e+02, -6.46069094e+02, -9.13891783e+02,  4.26878986e+02,\n",
       "       -7.18549936e+02, -4.35766689e+02, -1.43985168e+02,  1.07597054e+02,\n",
       "        1.65658787e+02, -2.39121637e+02, -8.25954952e+02, -4.60697469e+02,\n",
       "       -8.97482739e+02, -5.01651094e+02, -3.30854753e+02, -1.42940986e+02,\n",
       "        6.03957453e+02, -2.26067520e+02, -9.16642853e+02, -1.54236253e+02,\n",
       "        2.49659179e+02,  7.26106179e+02, -1.58205881e+02, -6.63484742e+02,\n",
       "       -6.13205621e+02,  3.15878340e+01, -2.55950506e+02, -2.65420491e+02,\n",
       "       -4.12773959e+02,  5.09302200e+02,  3.49916742e+01, -1.31857068e+02,\n",
       "       -9.82508114e+02, -2.65931205e+02,  1.19611411e+02, -8.82530352e+02,\n",
       "        3.55679320e+02, -2.00572608e+02, -4.51625367e+02, -3.06352164e+02,\n",
       "       -9.22748663e+02, -6.02349388e+02, -4.59759846e+02,  1.71923642e+02,\n",
       "       -8.02267887e+02, -2.39618709e+02,  1.55729497e+02, -1.41673423e+03,\n",
       "       -1.56055114e+02,  2.43132798e+01, -6.60566236e+02,  5.12923850e+02,\n",
       "       -4.56583792e+02, -7.98120060e+02, -3.25117619e+02, -3.76398458e+02,\n",
       "        4.70739759e+02, -7.38151320e+02, -5.86859524e+02, -5.72733124e+02,\n",
       "       -6.24790286e+02, -6.94208989e+01,  3.11712259e+02, -1.09646268e+03,\n",
       "       -1.45319433e+02, -6.95855720e+02,  9.71436343e+02,  1.57885001e+02,\n",
       "       -1.85339376e+01, -5.31201092e+02, -2.83107902e+02, -3.38474500e+02,\n",
       "       -1.49826033e+02, -4.02189774e+02, -2.51264936e+02, -7.43768222e+02,\n",
       "       -2.20367378e+02, -3.23126814e+02, -4.08287994e+02, -4.14713807e+02,\n",
       "        1.43340190e+02, -7.76869312e+02, -4.08558487e+02, -4.16544885e+02,\n",
       "       -3.52236205e+02, -2.82185111e+02, -5.48010456e+02,  1.71578815e+02,\n",
       "        1.29102183e+03, -3.87636015e+02,  7.14535796e+02, -8.02685751e+02,\n",
       "        8.19163587e+01, -5.49896410e+02, -8.47298210e+02, -5.22597050e+02,\n",
       "       -4.32691402e+02, -1.41223444e+01, -4.29660929e+02, -1.26589325e+02,\n",
       "        3.90320893e+01, -5.18399130e+02,  2.76474577e+02,  8.24966651e+02,\n",
       "        3.94736165e+02,  1.05738798e+02, -3.72110358e+02, -4.05272053e+02,\n",
       "       -5.21432255e+02,  1.71553749e+03,  2.70678820e+02,  1.86446664e+02,\n",
       "        4.35466210e+02, -1.90117276e+02, -7.51632065e+02, -1.85417732e+02,\n",
       "       -6.03085521e+02, -3.69985517e+02,  1.03894865e+04, -3.59102786e+02,\n",
       "        1.30302129e+02,  9.78952172e+02,  3.03726170e+01, -5.02907976e+02,\n",
       "       -7.52378861e+02,  2.23081371e+00,  7.70018975e+02, -1.86030157e+02,\n",
       "       -3.47777563e+02,  8.93724863e+02, -5.76173361e+02, -1.21568542e+02,\n",
       "       -2.01495387e-01,  2.33863811e+02, -3.88992450e+01,  5.02876714e+03])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = simulate_data(theta)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_statistics(data):\n",
    "    # Compute summary statistics of the data\n",
    "    # This function should return low-dimensional summary statistics S(data)\n",
    "    # For univariate alpha-stable models, you might use statistics like mean, variance, etc.\n",
    "    return np.mean(data), np.var(data)\n",
    "\n",
    "def smoothing_kernel(y, x, epsilon):\n",
    "    # Define smoothing kernel\n",
    "    # For example, let's use a Gaussian kernel\n",
    "    return np.exp(-np.linalg.norm(y - x) / (2 * epsilon**2)) / (np.sqrt(2 * np.pi) * epsilon)\n",
    "\n",
    "def likelihood_free_posterior(theta, data, epsilon):\n",
    "    # Compute likelihood-free posterior using Monte Carlo estimate\n",
    "    P = 1000  # Number of Monte Carlo samples\n",
    "    samples = [simulate_data(theta) for _ in range(P)]\n",
    "    summary_y = summary_statistics(data)\n",
    "    weights = [smoothing_kernel(summary_y, summary_statistics(x), epsilon) for x in samples]\n",
    "    weights /= np.sum(weights)  # Normalize weights\n",
    "    return np.mean(weights)\n",
    "\n",
    "def mutation_kernel(theta_t, theta_prev, weights_prev):\n",
    "    # Define mutation kernel based on the provided information\n",
    "    # For univariate alpha-stable models, use a Gaussian kernel with mean theta_prev[i] and covariance Λ\n",
    "    # Λ = diag(0.25, 0.25, 1, 1)\n",
    "    covariance = np.diag([0.25, 0.25, 1, 1])\n",
    "    new_theta = np.zeros_like(theta_t)\n",
    "    for i in range(len(weights_prev)):\n",
    "        new_theta += weights_prev[i] * multivariate_normal(mean=theta_prev[i], cov=covariance).pdf(theta_t)\n",
    "    return new_theta\n",
    "\n",
    "def resample(weights):\n",
    "    # Resampling step\n",
    "    indexes = np.random.choice(range(len(weights)), size=len(weights), p=weights)\n",
    "    return indexes\n",
    "\n",
    "def compute_weights(theta, data, t, ct, epsilon):\n",
    "    # Compute weights and rejection step\n",
    "    lf_posterior = likelihood_free_posterior(theta, data, epsilon)\n",
    "    mutation = mutation_kernel(theta)\n",
    "    weight = lf_posterior / mutation\n",
    "    rejection_prob = 1 - min(1, weight / ct)\n",
    "    if np.random.rand() < rejection_prob:\n",
    "        return None, None  # Reject sample\n",
    "    else:\n",
    "        return weight, weight / (1 - rejection_prob)\n",
    "\n",
    "def main():\n",
    "    T = 10  # Number of iterations\n",
    "    N = 1000  # Number of particles\n",
    "    epsilons = np.linspace(0.1, 0.01, T)  # Tolerance schedule\n",
    "\n",
    "    # Simulate observed data\n",
    "    observed_data = levy_stable.rvs(1.7, 0.9, loc=10, scale=10, size=200)\n",
    "\n",
    "    # Initialize weights and particles\n",
    "    weights = np.ones(N) / N\n",
    "    particles = [prior_sample() for _ in range(N)]\n",
    "\n",
    "    for t in range(T):\n",
    "        epsilon = epsilons[t]\n",
    "\n",
    "        # Resample particles\n",
    "        indexes = resample(weights)\n",
    "\n",
    "        # Mutation and correction step\n",
    "        for i in range(N):\n",
    "            theta_prev = particles[indexes[i]]\n",
    "            theta = mutation_kernel(theta_prev)\n",
    "            weight, corrected_weight = compute_weights(theta, observed_data, t, np.quantile(weights, 0.9), epsilon)\n",
    "            if weight is not None:\n",
    "                particles[i] = theta\n",
    "                weights[i] = corrected_weight\n",
    "\n",
    "    # Final weighted sample\n",
    "    final_particles = [(weights[i], particles[i]) for i in range(N)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def M_t(theta, theta_prev, weights_prev):\n",
    "    covariance = np.diag([0.25, 0.25, 1, 1])\n",
    "    return np.sum(weights_prev * multivariate_normal(mean=theta_prev, cov=covariance).pdf(theta))\n",
    "\n",
    "def generate_inverse_M_samples(N, theta_prev, weights_prev, covariance):\n",
    "    samples = []\n",
    "    while len(samples) < N:\n",
    "        # Generate random sample from the domain of theta\n",
    "        theta_sample = np.random.multivariate_normal(mean=theta_prev, cov=covariance)\n",
    "        # Generate a uniform random value between 0 and 1\n",
    "        u = np.random.uniform(0, 1)\n",
    "        # Evaluate the density function M at the sampled point\n",
    "        density_value = M_t(theta_sample, theta_prev, weights_prev)\n",
    "        # If the random value is less than the density value, accept the sample\n",
    "        if u < density_value:\n",
    "            samples.append(theta_sample)\n",
    "    return np.array(samples)\n",
    "\n",
    "# Example usage:\n",
    "N = 1000  # Number of random variables to generate\n",
    "theta_prev = np.array([0, 0, 0, 0])  # Previous theta values\n",
    "weights_prev = np.ones(10) / 10  # Previous weights\n",
    "covariance = np.diag([0.25, 0.25, 1, 1])  # Covariance matrix\n",
    "inverse_M_samples = generate_inverse_M_samples(N, theta_prev, weights_prev, covariance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13991764,  0.04722806,  0.07368945,  0.87503529],\n",
       "       [-0.38607345,  0.42447013,  1.17140437,  0.83137058],\n",
       "       [-0.16274022, -0.05762725,  1.0499936 , -1.18943621],\n",
       "       ...,\n",
       "       [-0.09867263, -0.12189777, -0.30388989,  0.82184195],\n",
       "       [ 0.63051267,  0.37264539, -0.32275047, -0.0385322 ],\n",
       "       [-0.01871269,  0.16135331,  1.32153095, -0.69370535]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_M_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
